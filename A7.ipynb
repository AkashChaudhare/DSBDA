{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\r0hn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\r0hn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\r0hn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\r0hn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't do computers but I sipped high tech\n",
    "#I don't do computers but my chips like that\n",
    "#I don't do computers but my shooters put a bullet wound in ya apple\n",
    "#It look bit like Mac\n",
    "\n",
    "text= \"I don't do computers but I sipped high tech. I don't do computers but my chips like that. I don't do computers but my shooters put a bullet wound in ya apple, It look bit like Mac\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I don't do computers but I sipped high tech.\", \"I don't do computers but my chips like that.\", \"I don't do computers but my shooters put a bullet wound in ya apple, It look bit like Mac\"]\n",
      "['I', 'do', \"n't\", 'do', 'computers', 'but', 'I', 'sipped', 'high', 'tech', '.', 'I', 'do', \"n't\", 'do', 'computers', 'but', 'my', 'chips', 'like', 'that', '.', 'I', 'do', \"n't\", 'do', 'computers', 'but', 'my', 'shooters', 'put', 'a', 'bullet', 'wound', 'in', 'ya', 'apple', ',', 'It', 'look', 'bit', 'like', 'Mac']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text= sent_tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "#Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['i', 'don', 't', 'do', 'computers', 'but', 'i', 'sipped', 'high', 'tech', 'i', 'don', 't', 'do', 'computers', 'but', 'my', 'chips', 'like', 'that', 'i', 'don', 't', 'do', 'computers', 'but', 'my', 'shooters', 'put', 'a', 'bullet', 'wound', 'in', 'ya', 'apple', 'it', 'look', 'bit', 'like', 'mac'] \n",
      "\n",
      "Filterd Sentence: ['computers', 'sipped', 'high', 'tech', 'computers', 'chips', 'like', 'computers', 'shooters', 'put', 'bullet', 'wound', 'ya', 'apple', 'look', 'bit', 'like', 'mac']\n"
     ]
    }
   ],
   "source": [
    "#stop word removal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "text= re.sub('[^a-zA-Z]', ' ',text)\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "filtered_text=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_text.append(w)\n",
    "print(\"Tokenized Sentence:\",tokens,\"\\n\")\n",
    "print(\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comput', 'sip', 'high', 'tech', 'comput', 'chip', 'like', 'comput', 'shooter', 'put', 'bullet', 'wound', 'ya', 'appl', 'look', 'bit', 'like', 'mac']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps =PorterStemmer()\n",
    "rootWords = []\n",
    "for w in filtered_text:\n",
    "    rootWord=ps.stem(w)\n",
    "    rootWords.append(rootWord)\n",
    "print(rootWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for i is i\n",
      "Lemma for don is don\n",
      "Lemma for t is t\n",
      "Lemma for do is do\n",
      "Lemma for computers is computer\n",
      "Lemma for but is but\n",
      "Lemma for i is i\n",
      "Lemma for sipped is sipped\n",
      "Lemma for high is high\n",
      "Lemma for tech is tech\n",
      "Lemma for i is i\n",
      "Lemma for don is don\n",
      "Lemma for t is t\n",
      "Lemma for do is do\n",
      "Lemma for computers is computer\n",
      "Lemma for but is but\n",
      "Lemma for my is my\n",
      "Lemma for chips is chip\n",
      "Lemma for like is like\n",
      "Lemma for that is that\n",
      "Lemma for i is i\n",
      "Lemma for don is don\n",
      "Lemma for t is t\n",
      "Lemma for do is do\n",
      "Lemma for computers is computer\n",
      "Lemma for but is but\n",
      "Lemma for my is my\n",
      "Lemma for shooters is shooter\n",
      "Lemma for put is put\n",
      "Lemma for a is a\n",
      "Lemma for bullet is bullet\n",
      "Lemma for wound is wound\n",
      "Lemma for in is in\n",
      "Lemma for ya is ya\n",
      "Lemma for apple is apple\n",
      "Lemma for it is it\n",
      "Lemma for look is look\n",
      "Lemma for bit is bit\n",
      "Lemma for like is like\n",
      "Lemma for mac is mac\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenization = tokens\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NN')]\n",
      "[('don', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('do', 'VB')]\n",
      "[('computers', 'NNS')]\n",
      "[('but', 'CC')]\n",
      "[('i', 'NN')]\n",
      "[('sipped', 'VBD')]\n",
      "[('high', 'JJ')]\n",
      "[('tech', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('don', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('do', 'VB')]\n",
      "[('computers', 'NNS')]\n",
      "[('but', 'CC')]\n",
      "[('my', 'PRP$')]\n",
      "[('chips', 'NNS')]\n",
      "[('like', 'IN')]\n",
      "[('that', 'IN')]\n",
      "[('i', 'NN')]\n",
      "[('don', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('do', 'VB')]\n",
      "[('computers', 'NNS')]\n",
      "[('but', 'CC')]\n",
      "[('my', 'PRP$')]\n",
      "[('shooters', 'NNS')]\n",
      "[('put', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('bullet', 'NN')]\n",
      "[('wound', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('ya', 'NN')]\n",
      "[('apple', 'NN')]\n",
      "[('it', 'PRP')]\n",
      "[('look', 'NN')]\n",
      "[('bit', 'NN')]\n",
      "[('like', 'IN')]\n",
      "[('mac', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words = tokens\n",
    "for word in words:\n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = \"I don't do computers but I sipped high tech\"\n",
    "documentB = \"I don't do computers but my chips like that\"\n",
    "documentC = \"I don't do computers but my shooters put a bullet wound in ya apple\"\n",
    "documentD = \"It look bit like Mac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n",
    "bagOfWordsC = documentC.split(' ')\n",
    "bagOfWordsD = documentD.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my', 'apple', 'do', 'bullet', 'in', 'ya', 'look', 'but', 'Mac', 'shooters', 'tech', 'It', 'sipped', 'chips', 'wound', 'put', 'that', 'bit', 'high', 'like', 'a', \"don't\", 'computers', 'I'}\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB)).union(set(bagOfWordsC)).union(set(bagOfWordsD))\n",
    "print(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "numOfWordsD = dict.fromkeys(uniqueWords, 0)\n",
    "numOfWordsC = dict.fromkeys(uniqueWords, 0)\n",
    "\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "    \n",
    "for word in bagOfWordsC:\n",
    "    numOfWordsC[word] += 1\n",
    "    \n",
    "for word in bagOfWordsD:\n",
    "    numOfWordsD[word] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 0.0, 'apple': 0.0, 'do': 0.1111111111111111, 'bullet': 0.0, 'in': 0.0, 'ya': 0.0, 'look': 0.0, 'but': 0.1111111111111111, 'Mac': 0.0, 'shooters': 0.0, 'tech': 0.1111111111111111, 'It': 0.0, 'sipped': 0.1111111111111111, 'chips': 0.0, 'wound': 0.0, 'put': 0.0, 'that': 0.0, 'bit': 0.0, 'high': 0.1111111111111111, 'like': 0.0, 'a': 0.0, \"don't\": 0.1111111111111111, 'computers': 0.1111111111111111, 'I': 0.2222222222222222} \n",
      "\n",
      " {'my': 0.1111111111111111, 'apple': 0.0, 'do': 0.1111111111111111, 'bullet': 0.0, 'in': 0.0, 'ya': 0.0, 'look': 0.0, 'but': 0.1111111111111111, 'Mac': 0.0, 'shooters': 0.0, 'tech': 0.0, 'It': 0.0, 'sipped': 0.0, 'chips': 0.1111111111111111, 'wound': 0.0, 'put': 0.0, 'that': 0.1111111111111111, 'bit': 0.0, 'high': 0.0, 'like': 0.1111111111111111, 'a': 0.0, \"don't\": 0.1111111111111111, 'computers': 0.1111111111111111, 'I': 0.1111111111111111} \n",
      "\n",
      " {'my': 0.07142857142857142, 'apple': 0.07142857142857142, 'do': 0.07142857142857142, 'bullet': 0.07142857142857142, 'in': 0.07142857142857142, 'ya': 0.07142857142857142, 'look': 0.0, 'but': 0.07142857142857142, 'Mac': 0.0, 'shooters': 0.07142857142857142, 'tech': 0.0, 'It': 0.0, 'sipped': 0.0, 'chips': 0.0, 'wound': 0.07142857142857142, 'put': 0.07142857142857142, 'that': 0.0, 'bit': 0.0, 'high': 0.0, 'like': 0.0, 'a': 0.07142857142857142, \"don't\": 0.07142857142857142, 'computers': 0.07142857142857142, 'I': 0.07142857142857142} \n",
      "\n",
      " {'my': 0.0, 'apple': 0.0, 'do': 0.0, 'bullet': 0.0, 'in': 0.0, 'ya': 0.0, 'look': 0.2, 'but': 0.0, 'Mac': 0.2, 'shooters': 0.0, 'tech': 0.0, 'It': 0.2, 'sipped': 0.0, 'chips': 0.0, 'wound': 0.0, 'put': 0.0, 'that': 0.0, 'bit': 0.2, 'high': 0.0, 'like': 0.2, 'a': 0.0, \"don't\": 0.0, 'computers': 0.0, 'I': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    \n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "tfC = computeTF(numOfWordsC, bagOfWordsC)\n",
    "tfD = computeTF(numOfWordsD, bagOfWordsD)\n",
    "print(tfA,\"\\n\\n\",tfB,'\\n\\n',tfC,'\\n\\n',tfD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 0.6931471805599453,\n",
       " 'apple': 1.3862943611198906,\n",
       " 'do': 0.28768207245178085,\n",
       " 'bullet': 1.3862943611198906,\n",
       " 'in': 1.3862943611198906,\n",
       " 'ya': 1.3862943611198906,\n",
       " 'look': 1.3862943611198906,\n",
       " 'but': 0.28768207245178085,\n",
       " 'Mac': 1.3862943611198906,\n",
       " 'shooters': 1.3862943611198906,\n",
       " 'tech': 1.3862943611198906,\n",
       " 'It': 1.3862943611198906,\n",
       " 'sipped': 1.3862943611198906,\n",
       " 'chips': 1.3862943611198906,\n",
       " 'wound': 1.3862943611198906,\n",
       " 'put': 1.3862943611198906,\n",
       " 'that': 1.3862943611198906,\n",
       " 'bit': 1.3862943611198906,\n",
       " 'high': 1.3862943611198906,\n",
       " 'like': 0.6931471805599453,\n",
       " 'a': 1.3862943611198906,\n",
       " \"don't\": 0.28768207245178085,\n",
       " 'computers': 0.28768207245178085,\n",
       " 'I': 0.28768207245178085}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "                \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "        \n",
    "    return idfDict\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB, numOfWordsC, numOfWordsD])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 0.0, 'apple': 0.0, 'do': 0.03196467471686454, 'bullet': 0.0, 'in': 0.0, 'ya': 0.0, 'look': 0.0, 'but': 0.03196467471686454, 'Mac': 0.0, 'shooters': 0.0, 'tech': 0.15403270679109896, 'It': 0.0, 'sipped': 0.15403270679109896, 'chips': 0.0, 'wound': 0.0, 'put': 0.0, 'that': 0.0, 'bit': 0.0, 'high': 0.15403270679109896, 'like': 0.0, 'a': 0.0, \"don't\": 0.03196467471686454, 'computers': 0.03196467471686454, 'I': 0.06392934943372908} \n",
      "\n",
      " {'my': 0.07701635339554948, 'apple': 0.0, 'do': 0.03196467471686454, 'bullet': 0.0, 'in': 0.0, 'ya': 0.0, 'look': 0.0, 'but': 0.03196467471686454, 'Mac': 0.0, 'shooters': 0.0, 'tech': 0.0, 'It': 0.0, 'sipped': 0.0, 'chips': 0.15403270679109896, 'wound': 0.0, 'put': 0.0, 'that': 0.15403270679109896, 'bit': 0.0, 'high': 0.0, 'like': 0.07701635339554948, 'a': 0.0, \"don't\": 0.03196467471686454, 'computers': 0.03196467471686454, 'I': 0.03196467471686454} \n",
      "\n",
      " {'my': 0.049510512897138946, 'apple': 0.09902102579427789, 'do': 0.020548719460841487, 'bullet': 0.09902102579427789, 'in': 0.09902102579427789, 'ya': 0.09902102579427789, 'look': 0.0, 'but': 0.020548719460841487, 'Mac': 0.0, 'shooters': 0.09902102579427789, 'tech': 0.0, 'It': 0.0, 'sipped': 0.0, 'chips': 0.0, 'wound': 0.09902102579427789, 'put': 0.09902102579427789, 'that': 0.0, 'bit': 0.0, 'high': 0.0, 'like': 0.0, 'a': 0.09902102579427789, \"don't\": 0.020548719460841487, 'computers': 0.020548719460841487, 'I': 0.020548719460841487} \n",
      "\n",
      " {'my': 0.0, 'apple': 0.0, 'do': 0.0, 'bullet': 0.0, 'in': 0.0, 'ya': 0.0, 'look': 0.2772588722239781, 'but': 0.0, 'Mac': 0.2772588722239781, 'shooters': 0.0, 'tech': 0.0, 'It': 0.2772588722239781, 'sipped': 0.0, 'chips': 0.0, 'wound': 0.0, 'put': 0.0, 'that': 0.0, 'bit': 0.2772588722239781, 'high': 0.0, 'like': 0.13862943611198905, 'a': 0.0, \"don't\": 0.0, 'computers': 0.0, 'I': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "tfidfC = computeTFIDF(tfC, idfs)\n",
    "tfidfD = computeTFIDF(tfD, idfs)\n",
    "\n",
    "print(tfidfA,\"\\n\\n\",tfidfB,'\\n\\n',tfidfC,'\\n\\n',tfidfD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
